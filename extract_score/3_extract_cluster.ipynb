{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import glob\n",
    "import shutil\n",
    "import copy\n",
    "import csv\n",
    "\n",
    "import time\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPSDataset(Dataset):\n",
    "    def __init__(self, metadata, root_dir,transform1=None, transform2=None):\n",
    "        self.metadata = pd.read_csv(metadata).values\n",
    "        self.root_dir = root_dir\n",
    "        self.transform1 = transform1\n",
    "        self.transform2 = transform2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.metadata[idx][0])\n",
    "        image =  Image.open(img_name).convert('RGB')\n",
    "        if self.transform1:\n",
    "            img1 = self.transform1(image)\n",
    "        if self.transform2:\n",
    "            img2 = self.transform2(image)\n",
    "            return img1, img2, idx\n",
    "                \n",
    "        return img1, idx\n",
    "\n",
    "class AUGLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AUGLoss, self).__init__()\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        b = (x1 - x2)\n",
    "        b = b*b\n",
    "        b = b.sum(1)\n",
    "        b = torch.sqrt(b)\n",
    "        return b.sum()\n",
    "\n",
    "# Below codes are from Deep Clustering for Unsupervised Learning of Visual Features github code        \n",
    "def preprocess_features(npdata, pca=15):\n",
    "    _, ndim = npdata.shape\n",
    "    npdata =  npdata.astype('float32')\n",
    "\n",
    "    # Apply PCA-whitening with Faiss\n",
    "    mat = faiss.PCAMatrix (ndim, pca, eigen_power=-0.5)\n",
    "    mat.train(npdata)\n",
    "    assert mat.is_trained\n",
    "    npdata = mat.apply_py(npdata)\n",
    "\n",
    "    # L2 normalization\n",
    "    row_sums = np.linalg.norm(npdata, axis=1)\n",
    "    npdata = npdata / row_sums[:, np.newaxis]\n",
    "\n",
    "    return npdata\n",
    "\n",
    "def cluster_assign(images_lists, dataset):\n",
    "    assert images_lists is not None\n",
    "    pseudolabels = []\n",
    "    image_indexes = []\n",
    "    for cluster, images in enumerate(images_lists):\n",
    "        image_indexes.extend(images)\n",
    "        pseudolabels.extend([cluster] * len(images))\n",
    "\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    t = transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                            transforms.RandomHorizontalFlip(),\n",
    "                            transforms.ToTensor(),\n",
    "                            normalize])\n",
    "\n",
    "    return ReassignedDataset(image_indexes, pseudolabels, dataset, t)\n",
    "\n",
    "\n",
    "def run_kmeans(x, nmb_clusters):\n",
    "    n_data, d = x.shape\n",
    "\n",
    "    # faiss implementation of k-means\n",
    "    clus = faiss.Clustering(d, nmb_clusters)\n",
    "\n",
    "    # Change faiss seed at each k-means so that the randomly picked\n",
    "    # initialization centroids do not correspond to the same feature ids\n",
    "    # from an epoch to another.\n",
    "    clus.seed = np.random.randint(1234)\n",
    "\n",
    "    clus.niter = 20\n",
    "    clus.max_points_per_centroid = 10000000\n",
    "    res = faiss.StandardGpuResources()\n",
    "    flat_config = faiss.GpuIndexFlatConfig()\n",
    "    flat_config.useFloat16 = False\n",
    "    flat_config.device = 0\n",
    "    index = faiss.GpuIndexFlatL2(res, d, flat_config)\n",
    "\n",
    "    # perform the training\n",
    "    clus.train(x, index)\n",
    "    _, I = index.search(x, 1)\n",
    "    stats = clus.iteration_stats\n",
    "    losses = np.array([stats.at(i).obj for i in range(stats.size())])\n",
    "    print('k-means loss evolution: {0}'.format(losses))\n",
    "\n",
    "    return [int(n[0]) for n in I], losses[-1]\n",
    "\n",
    "\n",
    "def compute_features(dataloader, model, N, batch_size):\n",
    "    model.eval()\n",
    "    # discard the label information in the dataloader\n",
    "    for i, (inputs, _) in enumerate(dataloader):\n",
    "        inputs = inputs.cuda()\n",
    "        aux = model(inputs).data.cpu().numpy()\n",
    "        aux = aux.reshape(-1, 1280)\n",
    "        if i == 0:\n",
    "            features = np.zeros((N, aux.shape[1]), dtype='float32')\n",
    "\n",
    "        aux = aux.astype('float32')\n",
    "        if i < len(dataloader) - 1:\n",
    "            features[i * batch_size: (i + 1) * batch_size] = aux\n",
    "        else:\n",
    "            features[i * batch_size:] = aux\n",
    "\n",
    "    return features  \n",
    "\n",
    "\n",
    "class Kmeans(object):\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "\n",
    "    def cluster(self, data,pca):\n",
    "        end = time.time()\n",
    "\n",
    "        # PCA-reducing, whitening and L2-normalization\n",
    "        xb = preprocess_features(data,pca)\n",
    "\n",
    "        # cluster the data\n",
    "        I, loss = run_kmeans(xb, self.k)\n",
    "        self.images_lists = [[] for i in range(self.k)]\n",
    "        label = []\n",
    "        for i in range(len(data)):\n",
    "            label.append(I[i])\n",
    "            self.images_lists[I[i]].append(i)\n",
    "            \n",
    "        label = torch.tensor(label).cuda()\n",
    "        print(label)\n",
    "\n",
    "        print('k-means time: {0:.0f} s'.format(time.time() - end))\n",
    "\n",
    "        return loss, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## label0 nature\n",
    "convnet = torch.load('/home/haoying/res_zl12_effnet_b0_9.7km/label0_pretrained.pt')\n",
    "convnet._fc = nn.Identity()\n",
    "convnet._swish = nn.Identity()\n",
    "# model = nn.Sequential(*(list(model.children())[:-3])) # strips off last linear layer\n",
    "convnet = torch.nn.DataParallel(convnet)    \n",
    "convnet.cuda()\n",
    "cluster_transform =transforms.Compose([\n",
    "                  transforms.Resize(256),\n",
    "                  transforms.CenterCrop(224),\n",
    "                  transforms.ToTensor(),\n",
    "                  transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7794, 1280)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusterset = GPSDataset('/home/haoying/res_zl12_effnet_b0_9.7km/nightlights_labeled0.csv', '/home/haoying/data_zl12/', cluster_transform)\n",
    "clusterloader = torch.utils.data.DataLoader(clusterset, batch_size=10, shuffle=False, num_workers=0)\n",
    "deepcluster = Kmeans(5)\n",
    "features = compute_features(clusterloader, convnet, len(clusterset), 10) \n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7794, 29)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_ = features\n",
    "pca = PCA(n_components = 0.80) \n",
    "pca.fit(X_)\n",
    "reduced_X = pca.transform(X_)\n",
    "reduced_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-means loss evolution: [3811.59912109 2174.92114258 2114.59790039 2084.4362793  2063.80126953\n",
      " 2054.59082031 2050.62060547 2047.18481445 2043.0802002  2039.55114746\n",
      " 2035.70031738 2030.75158691 2025.87719727 2022.05273438 2019.81616211\n",
      " 2018.20996094 2016.88012695 2016.07922363 2015.67749023 2015.49743652]\n",
      "tensor([2, 2, 1,  ..., 2, 3, 4], device='cuda:0')\n",
      "k-means time: 1 s\n"
     ]
    }
   ],
   "source": [
    "clustering_loss, p_label = deepcluster.cluster(features,pca=29)\n",
    "labels = p_label.tolist()\n",
    "f = open('/home/haoying/res_zl12_effnet_b0_9.7km/nightlights_labeled0.csv', 'r', encoding='utf-8')\n",
    "images = []\n",
    "rdr = csv.reader(f)\n",
    "for line in rdr:\n",
    "    images.append(line[0])\n",
    "f.close()\n",
    "images.pop(0)    \n",
    "nature_cluster = []\n",
    "for i in range(0, len(images)):\n",
    "    nature_cluster.append([images[i], labels[i]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "## label1 rurul\n",
    "convnet = torch.load('/home/haoying/res_zl12_effnet_b0_9.7km/label1_pretrained.pt')\n",
    "convnet._fc = nn.Identity()\n",
    "convnet._swish = nn.Identity()\n",
    "# model = nn.Sequential(*(list(model.children())[:-3])) # strips off last linear layer\n",
    "convnet = torch.nn.DataParallel(convnet)    \n",
    "convnet.cuda()\n",
    "cluster_transform =transforms.Compose([\n",
    "                  transforms.Resize(256),\n",
    "                  transforms.CenterCrop(224),\n",
    "                  transforms.ToTensor(),\n",
    "                  transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21672, 1280)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusterset = GPSDataset('/home/haoying/res_zl12_effnet_b0_9.7km/nightlights_labeled1.csv', '/home/haoying/data_zl12/', cluster_transform)\n",
    "clusterloader = torch.utils.data.DataLoader(clusterset, batch_size=10, shuffle=False, num_workers=0)\n",
    "deepcluster = Kmeans(5)\n",
    "features = compute_features(clusterloader, convnet, len(clusterset), 10) \n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21672, 12)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ = features\n",
    "pca = PCA(n_components = 0.80) \n",
    "pca.fit(X_)\n",
    "reduced_X = pca.transform(X_)\n",
    "reduced_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-means loss evolution: [14894.32519531  9277.61425781  8862.14355469  8615.33496094\n",
      "  8441.54492188  8320.85546875  8249.43652344  8207.28320312\n",
      "  8176.56982422  8156.80322266  8146.30566406  8140.29882812\n",
      "  8136.93554688  8134.04736328  8130.34667969  8123.66699219\n",
      "  8109.65625     8078.0859375   8038.23876953  8013.61132812]\n",
      "tensor([0, 2, 0,  ..., 0, 1, 0], device='cuda:0')\n",
      "k-means time: 1 s\n"
     ]
    }
   ],
   "source": [
    "clustering_loss, p_label = deepcluster.cluster(features,pca=12)\n",
    "labels = p_label.tolist()\n",
    "f = open('/home/haoying/res_zl12_effnet_b0_9.7km/nightlights_labeled1.csv', 'r', encoding='utf-8')\n",
    "images = []\n",
    "rdr = csv.reader(f)\n",
    "for line in rdr:\n",
    "    images.append(line[0])\n",
    "f.close()\n",
    "images.pop(0)    \n",
    "rurul_cluster = []\n",
    "for i in range(0, len(images)):\n",
    "    rurul_cluster.append([images[i], labels[i]+5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## label2 city\n",
    "convnet = torch.load('/home/haoying/res_zl12_effnet_b0_9.7km/label2_pretrained.pt')\n",
    "convnet._fc = nn.Identity()\n",
    "convnet._swish = nn.Identity()\n",
    "# model = nn.Sequential(*(list(model.children())[:-3])) # strips off last linear layer\n",
    "convnet = torch.nn.DataParallel(convnet)    \n",
    "convnet.cuda()\n",
    "cluster_transform =transforms.Compose([\n",
    "                  transforms.Resize(256),\n",
    "                  transforms.CenterCrop(224),\n",
    "                  transforms.ToTensor(),\n",
    "                  transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9500, 1280)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusterset = GPSDataset('/home/haoying/res_zl12_effnet_b0_9.7km/nightlights_labeled2.csv', '/home/haoying/data_zl12/', cluster_transform)\n",
    "clusterloader = torch.utils.data.DataLoader(clusterset, batch_size=10, shuffle=False, num_workers=0)\n",
    "deepcluster = Kmeans(3)\n",
    "features = compute_features(clusterloader, convnet, len(clusterset), 10) \n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9500, 37)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ = features\n",
    "pca = PCA(n_components = 0.80) \n",
    "pca.fit(X_)\n",
    "reduced_X = pca.transform(X_)\n",
    "reduced_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-means loss evolution: [8856.29003906 5882.54736328 5577.25244141 5484.26269531 5430.47412109\n",
      " 5407.3671875  5401.48046875 5399.93017578 5399.75976562 5399.71289062\n",
      " 5399.70361328 5399.70507812 5399.70507812 5399.70507812 5399.70507812\n",
      " 5399.70507812 5399.70507812 5399.70507812 5399.70507812 5399.70507812]\n",
      "tensor([1, 1, 2,  ..., 1, 1, 2], device='cuda:0')\n",
      "k-means time: 1 s\n"
     ]
    }
   ],
   "source": [
    "clustering_loss, p_label = deepcluster.cluster(features,pca=37)\n",
    "labels = p_label.tolist()\n",
    "f = open('/home/haoying/res_zl12_effnet_b0_9.7km/nightlights_labeled2.csv', 'r', encoding='utf-8')\n",
    "images = []\n",
    "rdr = csv.reader(f)\n",
    "for line in rdr:\n",
    "    images.append(line[0])\n",
    "f.close()\n",
    "images.pop(0)    \n",
    "city_cluster = []\n",
    "for i in range(0, len(images)):\n",
    "    city_cluster.append([images[i], labels[i]+10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_city_cluster():\n",
    "#     convnet = torch.load('/home/haoying/res_zl12_effnet_b0_9.7km/label2_pretrained.pt')\n",
    "#     convnet = torch.nn.DataParallel(convnet)    \n",
    "#     convnet.cuda()\n",
    "#     cluster_transform =transforms.Compose([\n",
    "#                       transforms.Resize(256),\n",
    "#                       transforms.CenterCrop(224),\n",
    "#                       transforms.ToTensor(),\n",
    "#                       transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])    \n",
    "    \n",
    "#     clusterset = GPSDataset('/home/haoying/res_zl12_effnet_b0_9.7km/nightlights_labeled2.csv', '/home/haoying/data_zl12/', cluster_transform)\n",
    "#     clusterloader = torch.utils.data.DataLoader(clusterset, batch_size=10, shuffle=False, num_workers=0)\n",
    "    \n",
    "#     deepcluster = Kmeans(5)\n",
    "#     features = compute_features(clusterloader, convnet, len(clusterset), 10) \n",
    "#     clustering_loss, p_label = deepcluster.cluster(features,pca=11)\n",
    "#     labels = p_label.tolist()\n",
    "#     f = open('/home/haoying/res_zl12_effnet_b0_9.7km/nightlights_labeled2.csv', 'r', encoding='utf-8')\n",
    "#     images = []\n",
    "#     rdr = csv.reader(f)\n",
    "#     for line in rdr:\n",
    "#         images.append(line[0])\n",
    "#     f.close()\n",
    "#     images.pop(0)    \n",
    "#     city_cluster = []\n",
    "#     for i in range(0, len(images)):\n",
    "#         city_cluster.append([images[i], labels[i]]) \n",
    "        \n",
    "#     return city_cluster\n",
    "\n",
    "# def extract_rural_cluster():\n",
    "#     convnet = torch.load('/home/haoying/res_zl12_effnet_b0_9.7km/label1_pretrained.pt')\n",
    "#     convnet = torch.nn.DataParallel(convnet)    \n",
    "#     convnet.cuda()\n",
    "#     cluster_transform =transforms.Compose([\n",
    "#                       transforms.Resize(256),\n",
    "#                       transforms.CenterCrop(224),\n",
    "#                       transforms.ToTensor(),\n",
    "#                       transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])    \n",
    "    \n",
    "#     clusterset = GPSDataset('/home/haoying/res_zl12_effnet_b0_9.7km/nightlights_labeled1.csv', '/home/haoying/data_zl12/', cluster_transform)\n",
    "#     clusterloader = torch.utils.data.DataLoader(clusterset, batch_size=10, shuffle=False, num_workers=0)\n",
    "    \n",
    "#     deepcluster = Kmeans(5)\n",
    "#     features = compute_features(clusterloader, convnet, len(clusterset), 10) \n",
    "#     clustering_loss, p_label = deepcluster.cluster(features,pca=6)\n",
    "#     labels = p_label.tolist()\n",
    "#     f = open('/home/haoying/res_zl12_effnet_b0_9.7km/nightlights_labeled1.csv', 'r', encoding='utf-8')\n",
    "#     images = []\n",
    "#     rdr = csv.reader(f)\n",
    "#     for line in rdr:\n",
    "#         images.append(line[0])\n",
    "#     f.close()\n",
    "#     images.pop(0)    \n",
    "#     rural_cluster = []\n",
    "#     for i in range(0, len(images)):\n",
    "#         rural_cluster.append([images[i], labels[i] + 5])\n",
    "        \n",
    "#     return rural_cluster\n",
    "\n",
    "# def extract_nature_cluster():\n",
    "#     convnet = torch.load('/home/haoying/res_zl12_effnet_b0_9.7km/label0_pretrained.pt')\n",
    "#     convnet = torch.nn.DataParallel(convnet)    \n",
    "#     convnet.cuda()\n",
    "#     cluster_transform =transforms.Compose([\n",
    "#                       transforms.Resize(256),\n",
    "#                       transforms.CenterCrop(224),\n",
    "#                       transforms.ToTensor(),\n",
    "#                       transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])    \n",
    "    \n",
    "#     clusterset = GPSDataset('/home/haoying/res_zl12_effnet_b0_9.7km/nightlights_labeled0.csv', '/home/haoying/data_zl12/', cluster_transform)\n",
    "#     clusterloader = torch.utils.data.DataLoader(clusterset, batch_size=10, shuffle=False, num_workers=0)\n",
    "    \n",
    "#     deepcluster = Kmeans(8)\n",
    "#     features = compute_features(clusterloader, convnet, len(clusterset), 10) \n",
    "#     clustering_loss, p_label = deepcluster.cluster(features,pca=10)\n",
    "#     labels = p_label.tolist()\n",
    "#     f = open('/home/haoying/res_zl12_effnet_b0_9.7km/nightlights_labeled0.csv', 'r', encoding='utf-8')\n",
    "#     images = []\n",
    "#     rdr = csv.reader(f)\n",
    "#     for line in rdr:\n",
    "#         images.append(line[0])\n",
    "#     f.close()\n",
    "#     images.pop(0)    \n",
    "#     nature_cluster = []\n",
    "#     for i in range(0, len(images)):\n",
    "#         nature_cluster.append([images[i], labels[i] + 10])\n",
    "        \n",
    "#     return nature_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# city_cluster = extract_city_cluster()\n",
    "# rural_cluster = extract_rural_cluster()\n",
    "# nature_cluster = extract_nature_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cluster = city_cluster + rurul_cluster + nature_cluster\n",
    "cnum = 13\n",
    "cluster_dir = '/home/haoying/res_zl12_effnet_b0_9.7km/data/'\n",
    "if not os.path.exists(cluster_dir):\n",
    "    os.makedirs(cluster_dir)\n",
    "for i in range(0, cnum + 1):\n",
    "    os.makedirs(cluster_dir + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cluster = city_cluster + rurul_cluster + nature_cluster\n",
    "df = pd.DataFrame(total_cluster,columns=['y_x','cluster_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cluster_id\n",
       "0       411\n",
       "1      1008\n",
       "2      5423\n",
       "3       539\n",
       "4       413\n",
       "5     11769\n",
       "6      3335\n",
       "7      2561\n",
       "8      2205\n",
       "9      1802\n",
       "10     1396\n",
       "11     3682\n",
       "12     4422\n",
       "Name: y_x, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('cluster_id')['y_x'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, cnum):\n",
    "    path='/home/haoying/res_zl12_effnet_b0_9.7km/data/'+str(i)+'/cluster.csv'\n",
    "    df[df['cluster_id']==i].to_csv(path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/home/haoying/res_zl12_effnet_b0_9.7km/data/unified.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
